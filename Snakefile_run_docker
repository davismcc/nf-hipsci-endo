"""
Snakefile for one lane of single-cell fibroblast project (Raghd Rostom)

Author: Davis McCarthy
Affiliation: EMBL-EBI

Run from run directory
Run: snakemake -s ../../../Snakefile_run_docker --jobs 2000 --latency-wait 30 --cluster-config ../../../cluster.json --cluster 'bsub -J {cluster.name} -q {cluster.queue} -n {cluster.n} -R "rusage[mem={cluster.memory}]" -M {cluster.memory}  -o {cluster.output} -e {cluster.error}'

Davis McCarthy, 20 November 2017
"""

import glob
import os
from subprocess import run
import subprocess
import pandas as pd
import re

shell.prefix("set -euo pipefail;")

## singularity image with installed software
sing_cmd = 'singularity exec ../../../endo.img '

## reference files
references_dir = '../../../references/'
human_tx_fasta = 'Homo_sapiens.GRCh37.rel75.cdna.all.ERCC92.fa.gz'
human_gx_fasta = os.path.join(references_dir, 'STAR_GRCh37.75_ERCC/GRCh37.p13.genome.ERCC92.fa')
fasta = os.path.join(references_dir, 'human_tx_fasta')
fasta_unzipped = human_gx_fasta
fasta_dict = fasta_unzipped.replace('fa', 'dict')
fasta_idx = fasta_unzipped + '.fai'
dbSnpVcf = os.path.join(references_dir, 'dbsnp_138.hg19.vcf.gz')
dbSnpVcfSmall = os.path.join(references_dir, 'dbsnp_138.hg19.biallelicSNPs.HumanCoreExome12.Top1000ExpressedIpsGenes.Maf0.01.HWE0.0001.HipSci.vcf.gz')
reAlignmentIntervals = os.path.join(references_dir, 'knownIndels.intervals')
knownIndelsMills = os.path.join(references_dir, '1000G_phase1.indels.hg19.sites.vcf.gz')
knownIndels100G = os.path.join(references_dir, 'Mills_and_1000G_gold_standard.indels.hg19.sites.vcf.gz')
## fasta files for GRCh38
fasta_GRCh38 = os.path.join(references_dir, 'Homo_sapiens.GRCh38.rel85.cdna.all.ERCC92.fa.gz')
fasta_unzipped_GRCh38 = os.path.join(references_dir, 'Homo_sapiens.GRCh38.dna.primary_assembly.ERCC92.fa')
refflat_file = os.path.join(references_dir, 'Homo_sapiens.GRCh37.75_ERCC.ref_flat.txt')
HIPSCI_VCF = os.path.join(references_dir, 'hipsci.wec.gtarray.HumanCoreExome.imputed_phased.20170327.genotypes.allchr.vcf.gz')
HIPSCI_VCF_GNOMAD = os.path.join(references_dir, 'hipsci.wec.gtarray.HumanCoreExome.imputed_phased.INFO_0.4_filtered.20170327.genotypes.allchr.gnomad.exomes.common.biallelic.contig_reordered.vcf.gz')
anno_gencode_v19 = os.path.join(references_dir, 'gencode.v19.annotation_ERCC.gtf')
salmon_idx = os.path.join(references_dir, 'Homo_sapiens.GRCh37.rel75.cdna.all.ERCC92.salmon_v0.8.2_idx')
salmon_idx_GRCh38 = os.path.join(references_dir, 'Homo_sapiens.GRCh38.rel85.cdna.all.ERCC92.salmon_v0.8.2_idx')
annos_featurecounts = os.path.join(references_dir, 'Homo_sapiens.GRCh37.75_c_hg19_rmsk_TE_ensemblChrmapping.filtered.hipsciexpressed.gtf')
targets_callvars = os.path.join(references_dir, 'dbsnp_138.hg19.biallelicSNPs.HumanCoreExome12.Top1000ExpressedIpsGenes.Maf0.01.HWE0.0001.HipSci.als.tsv.gz')

## parameter objects and samples
GENOME = 'genome.fa'
STAR_GENOME_DIR = 'references/STAR_GRCh37.75_ERCC'
star_genome_files = ['chrLength.txt', 'chrNameLength.txt', 'chrName.txt', 'chrStart.txt', 'exonInfo.tab', 'Genome', 'genomeParameters.txt', 'SA', 'SAindex', 'sjdbInfo.txt', 'sjdbList.fromGTF.out.tab', 'sjdbList.out.tab', 'transcriptInfo.tab']

## define commands
bcftools_cmd = 'singularity exec ../../../endo.img bcftools'
featurecounts_cmd = 'singularity exec ../../../endo.img featureCounts'
#gatk_cmd = 'singularity exec ../../../endo.img gatk'
gatk_cmd = 'singularity exec ../../../gatk.img' # gatk 4
#leafcutter_cmd = 'sh /hps/nobackup/stegle/users/mjbonder/tools/leafcutter/scripts/bam2junc.sh'
picard_cmd = 'singularity exec ../../../endo.img picard'
python_cmd = 'singularity exec ../../../endo.img python'
rscript_cmd = 'singularity exec ../../../endo.img Rscript'
salmon_cmd = 'singularity exec ../../../endo.img salmon'
samtools_cmd = 'singularity exec ../../../endo.img samtools'
star_cmd = 'singularity exec ../../../endo.img STAR'
tabix_cmd = 'singularity exec ../../../endo.img tabix'
trim_galore_cmd = 'singularity exec ../../../endo.img trim_galore'
## scripts
read_salmon_to_scesets_cmd = '../../../src/preprocessing/read_salmon_to_scesets.R'

## targets
star_genome_output = expand('{genome_dir}/{genome_files}', genome_dir=STAR_GENOME_DIR, genome_files=star_genome_files)

## read in crams from SS2 run
crams_all = glob.glob('cram/*.cram')

## define sample names
SAMPLES = [os.path.basename(w).replace('.cram', '') for w in crams_all]
gatk_split_bam_output = []
picard_QC_output = []
leaf_cutter_junc = []
feature_counts_output = []

fastqc_html_reports = expand('fastqc/{sample}.2pass.Aligned.sortedByCoord.out_fastqc.html', sample=SAMPLES)
cell_vcf_files = expand('vcf/{sample}/{sample}.filtered.vcf.gz', sample=SAMPLES)
variant_donor_id_files = expand('donor_id/{sample}.donor_id.csv', sample=SAMPLES)
donor_id_all_files = 'donor_id_all.csv'
cells_merged_vcfs = 'cells_merged.vcf.gz'
merged_bams = expand('bam/merged/{sample}.bam', sample=SAMPLES)
salmon_results_GRCh37 = expand('quant_salmon/{sample}/quant.sf', sample=SAMPLES)
salmon_results_GRCh38 = expand('quant_salmon_GRCh38/{sample}/quant.sf', sample=SAMPLES)
sces = expand('sces/sce.salmon.{genome}.preqc_tx.rds', genome=['GRCh37'])
sces.append(expand('sces/sce.salmon.{genome}.preqc_gene.rds', genome=['GRCh37']))
scater_first_html_reports = expand('first_qc/salmon.{genome}.first_qc.html', genome=['GRCh37'])
seq_metadata_files = expand('meta/{sample}.meta', sample=SAMPLES)
seq_metadata_output = "seq_metadata.tsv"
gatk_split_bam_output.append(expand('star/{sample}/{sample}.2pass.Aligned.sortedByCoord.split.bam', run=run, sample=SAMPLES))
picard_QC_output.append(expand('star/{sample}/{sample}.2pass.Aligned.sortedByCoord.dedup.jump_metrics', run=run, sample=SAMPLES))
picard_QC_output.append(expand('star/{sample}/{sample}.2pass.Aligned.sortedByCoord.out.rna_metrics', run=run, sample=SAMPLES))
leaf_cutter_junc.append(expand('star/{sample}/{sample}.2pass.Aligned.sortedByCoord.dedup.primary.bam.junc', run=run, sample=SAMPLES))
feature_counts_output.append(expand('featureCounts/unique_counts/{sample}.gene.counts.unique.tsv', run=run, sample=SAMPLES))
feature_counts_output.append(expand('featureCounts/unique_counts/{sample}.exon.counts.unique.tsv', run=run, sample=SAMPLES))


# hipsci_chr_vcf = []
# hipsci_chr_vcf_idx = []
# for i in range(1, 23):
#     hipsci_chr_vcf.append('/hps/nobackup/hipsci/scratch/genotypes/imputed/2017-03-27/Full/hipsci.wec.gtarray.HumanCoreExome.imputed_phased.20170327.genotypes.chr%s.vcf.gz' %str(i))
#     hipsci_chr_vcf_idx.append('/hps/nobackup/hipsci/scratch/genotypes/imputed/2017-03-27/Full/hipsci.wec.gtarray.HumanCoreExome.imputed_phased.20170327.genotypes.chr%s.vcf.gz.tbi' %str(i))

gatk_split_bam_output = [filename for elem in gatk_split_bam_output for filename in elem]
picard_QC_output = [filename for elem in picard_QC_output for filename in elem]
leaf_cutter_junc = [filename for elem in leaf_cutter_junc for filename in elem]
feature_counts_output = [filename for elem in feature_counts_output for filename in elem]


rule all:
    input:
        # HIPSCI_VCF,
        donor_id_all_files,
        "seq_metadata.tsv",
        sces,
        cells_merged_vcfs,
        picard_QC_output,
        feature_counts_output,
        leaf_cutter_junc


rule picard_CollectRnaSeqMetrics:
    input:
        'star/{sample}/{sample}.2pass.Aligned.sortedByCoord.out.bam'
    output:
        'star/{sample}/{sample}.2pass.Aligned.sortedByCoord.out.rna_metrics'
    shell:
        '{picard_cmd} CollectRnaSeqMetrics I={input} O={output} REF_FLAT={refflat_file} STRAND=NONE'


rule picard_CollectJumpingLibraryMetrics:
    input:
        'star/{sample}/{sample}.2pass.Aligned.sortedByCoord.dedup.bam'
    output:
        'star/{sample}/{sample}.2pass.Aligned.sortedByCoord.dedup.jump_metrics'
    shell:
        '{picard_cmd} CollectJumpingLibraryMetrics I={input} O={output}'


rule build_star_genome_indexes:
    input:
        fasta=fasta_unzipped,
        annotation=anno_gencode_v19
    output:
        star_genome_output
    threads: 8
    shell:
        '{star_cmd} --runMode genomeGenerate --genomeDir {STAR_GENOME_DIR} '
        '--genomeFastaFiles {input.fasta} --runThreadN {threads} '
        '--sjdbGTFfile {input.annotation} --sjdbOverhang 100'


rule create_fasta_index:
    input:
        fasta_unzipped
    output:
        fasta_idx
    shell:
        '{samtools_cmd} faidx {input}'


rule create_fasta_dict:
    input:
        fasta_unzipped
    output:
        fasta_dict
    shell:
        '{picard_cmd} CreateSequenceDictionary R={input} O={output}'


# rule create_vcf_file_list:
#     output:
#         'data_raw/Full-2017-03-29_genotype_vcf_file_list.txt'
#     run:
#         with open(output[0], "w+") as outfile:
#                 for i in range(1, 23):
#                         outfile.write('/hps/nobackup/hipsci/scratch/genotypes/imputed/2017-03-27/Full/hipsci.wec.gtarray.HumanCoreExome.imputed_phased.20170327.genotypes.chr%s.vcf.gz\n' %str(i))


rule generate_new_tabix_idx:
    input:
        hipsci_chr_vcf
    output:
        hipsci_chr_vcf_idx
    run:
        for chrom in hipsci_chr_vcf:
            print(chrom)
            cmd = '{tabix_cmd} -f -p vcf ' + chrom
            print(cmd)
            subprocess.run(cmd, shell = True)


rule create_hipsci_vcf:
    input:
        'data_raw/Full-2017-03-29_genotype_vcf_file_list.txt'
    output:
        vcf=HIPSCI_VCF,
        vcf_idx=HIPSCI_VCF + '.tbi'
    shell:
        """
        {bcftools_cmd} concat -f {input} -O z > {output.vcf}
        {tabix_cmd} -f -p vcf {output.vcf}
        """


rule build_salmon_index:
    input:
        fasta
    output:
        salmon_idx
    shell:
        '{salmon_cmd} index -t {input} -i {output} --type quasi -k 31 '
        '--perfectHash'

    
rule build_salmon_index_GRCh38:
    input:
        fasta_GRCh38
    output:
        salmon_idx_GRCh38
    shell:
        '{salmon_cmd} index -t {input} -i {output} --type quasi -k 31 '
        '--perfectHash'


rule seq_metadata_collect:
    input:
        files=seq_metadata_files
    output:
        'seq_metadata.tsv'
    params:
        dir='meta/',
    run:
        import os
        import glob
        import re
        import pandas as pd
        metafiles = glob.glob(os.path.join(params.dir, '*.meta'))
        seq_sample_names = [os.path.basename(x).replace(".meta", "") for x in metafiles]
        metadict = {}
        for fname in metafiles:
            tmpdict = {}
            with open(fname) as f:
                current_attribute = ''
                for line in f:
                    if re.search('^attribute', line):
                        current_attribute = line.replace('attribute: ', '').strip()
                    if re.search('^value', line):
                        tmpdict[current_attribute] = line.replace('value: ', '').strip()
                metadict[os.path.basename(fname).replace(".meta", "")] = tmpdict
        df = pd.DataFrame(metadict).transpose()
        df['cram_id'] = df.index
        df.to_csv(output[0], index=False, sep='\t')


rule cram2fastq:
    input:
        'cram/{sample}.cram'
    output:
        fq1=temp('fastq/{sample}_1.fastq'),
        fq2=temp('fastq/{sample}_2.fastq')
    shell:
        """
        {samtools_cmd} view -u {input} | \
        {samtools_cmd} collate -uOn 128 - tmp/tmp-prefix-{wildcards.sample} | \
        {samtools_cmd} fastq -F 0xB00 -1 {output.fq1} -2 {output.fq2} -
        """
        #'{samtools_cmd} sort -m 10G -n -T %s {input} | {samtools_cmd} fastq -F 0xB00 -1 {output.fq1} -2 {output.fq2} -'
        #'{samtools_cmd} fastq -t -1 {output.fq1} -2 {output.fq2} {input}'


rule trim_fastq_SS2:
    input:
        fq1="fastq/{sample}_1.fastq",
        fq2="fastq/{sample}_2.fastq"
    output:
        fq1='fastq/{sample}_1_val_1.fq.gz',
        fq2='fastq/{sample}_2_val_2.fq.gz'
    priority: 6
    shell:
        '{trim_galore_cmd} --gzip '
        '--output_dir fastq '
        '--length 40 --paired '
        '{input.fq1} {input.fq2}'


rule salmon_quant:
    input:
        sidx=salmon_idx,
        fq1='fastq/{sample}_1_val_1.fq.gz',
        fq2='fastq/{sample}_2_val_2.fq.gz'
    output:
        'quant_salmon/{sample}/quant.sf'
    threads: 4
    params:
        folder='quant_salmon/{sample}/'
    shell:
        '{salmon_cmd} quant -i {input.sidx} -l IU '
        '-1 {input.fq1} -2 {input.fq2} '
        '--seqBias --gcBias --threads {threads} --useVBOpt ' 
        '-o {params.folder}'


rule salmon_quant_GRCh38:
    input:
        sidx=salmon_idx_GRCh38,
        fq1='fastq/{sample}_1_val_1.fq.gz',
        fq2='fastq/{sample}_2_val_2.fq.gz'
    output:
        'quant_salmon_GRCh38/{sample}/quant.sf'
    threads: 4
    params:
        folder='quant_salmon_GRCh38/{sample}/'
    shell:
        '{salmon_cmd} quant -i {input.sidx} -l IU '
        '-1 {input.fq1} -2 {input.fq2} '
        '--seqBias --gcBias --threads {threads} --useVBOpt ' 
        '-o {params.folder}'


rule align_with_star_2pass_SS2:
    input:
        star_genome_output,
        genome_dir=STAR_GENOME_DIR,
        fq1='fastq/{sample}_1_val_1.fq.gz',
        fq2='fastq/{sample}_2_val_2.fq.gz'
    output:
        temp('star/{sample}/{sample}.2pass.Aligned.out.bam')
    params:
        prefix='star/{sample}/{sample}.2pass.'
    threads: 12
    shell:
        '{star_cmd} --genomeDir {input.genome_dir} '
        '--readFilesIn {input.fq1} {input.fq2} '
        '--outFileNamePrefix {params.prefix} '
        '--outSAMtype BAM Unsorted '
        '--alignSJoverhangMin 8 '
        '--alignSJDBoverhangMin 1 --genomeLoad NoSharedMemory '
        '--alignIntronMin 20 --alignIntronMax 1000000 '
        '--alignMatesGapMax 1000000 --sjdbScore 2 '
        '--outFilterType BySJout '
        '--outFilterMultimapNmax 20 --outFilterMismatchNmax 999 '
        '--outFilterMismatchNoverLmax 0.04 '
        '--outFilterScoreMinOverLread 0.33 --outFilterMatchNminOverLread 0.33 '
        '--outSAMstrandField intronMotif '
        '--outFilterIntronMotifs RemoveNoncanonical '
        '--outSAMattributes NH HI NM MD AS XS --outSAMunmapped Within '
        '--runThreadN {threads} --twopassMode Basic '
        '--readFilesCommand zcat '


rule sort_alignment:
    input:
        'star/{sample}/{sample}.2pass.Aligned.out.bam'
    output:
        'star/{sample}/{sample}.2pass.Aligned.sortedByCoord.out.bam'
    threads: 4
    shell:
        '{samtools_cmd} sort -m 12G -@ 4 -O bam -o {output} {input}'


rule index_star_bams:
    input:
        'star/{sample}/{sample}.2pass.Aligned.sortedByCoord.out.bam'
    output:
        temp('star/{sample}/{sample}.2pass.Aligned.sortedByCoord.out.bam.bai')
    shell:
        '{samtools_cmd} index {input} '


rule picard_read_groups:
    input:
        bam='star/{sample}/{sample}.2pass.Aligned.sortedByCoord.out.bam'
    output:
        temp('star/{sample}/{sample}.2pass.Aligned.sortedByCoord.rgadded.bam')
    shell:
        '{picard_cmd} AddOrReplaceReadGroups I={input.bam} O={output} SO=coordinate '
        'RGID={wildcards.sample} RGLB={wildcards.sample} '
        'RGPL=ILLUMINA RGPU=MACHINE1 RGSM={wildcards.sample}'


rule picard_mark_dups:
    input:
        'star/{sample}/{sample}.2pass.Aligned.sortedByCoord.rgadded.bam'
    output:
        bam='star/{sample}/{sample}.2pass.Aligned.sortedByCoord.dedup.bam',
        metrics='star/{sample}/{sample}.output.metrics'
    shell:
        '{picard_cmd} MarkDuplicates I={input} O={output.bam} CREATE_INDEX=true '
        'VALIDATION_STRINGENCY=SILENT M={output.metrics} REMOVE_DUPLICATES=true'


rule write_primary_alignment_bam:
    input:
        'star/{sample}/{sample}.2pass.Aligned.sortedByCoord.dedup.bam'
    output:
        bam_primary='star/{sample}/{sample}.2pass.Aligned.sortedByCoord.dedup.primary.bam'
    params:
        headerTmp = 'star/{sample}/header.sam'
    shell:
        '{samtools_cmd} view -H {input} > {params.headerTmp}; '
        '{samtools_cmd} view {input} | grep "NH:i:1" | cat {params.headerTmp} - | {samtools_cmd} view -Sb - > {output.bam_primary}; '
        'rm {params.headerTmp}'


rule create_junc_files:
    input:
        'star/{sample}/{sample}.2pass.Aligned.sortedByCoord.dedup.primary.bam'
    output:
        'star/{sample}/{sample}.2pass.Aligned.sortedByCoord.dedup.primary.bam.junc'
    shell:
        '{leafcutter_cmd} {input} {output} '


rule count_genes_unique:
    input:
        bam = 'star/{sample}/{sample}.2pass.Aligned.sortedByCoord.dedup.bam',
        annotation=annos_featurecounts
    output:
        'featureCounts/unique_counts/{sample}.gene.counts.unique.tsv'
    priority: 4
    shell:
        '{featurecounts_cmd} '
        ' -B -C -p --primary -R CORE'
        ' -a {input.annotation} -g gene_id '
        ' -o  {output} {input.bam}'


rule count_features_unique:
    input:
        bam = 'star/{sample}/{sample}.2pass.Aligned.sortedByCoord.dedup.bam',
        annotation=annos_featurecounts
    output:
        'featureCounts/unique_counts/{sample}.exon.counts.unique.tsv'
    priority: 4
    shell:
        '{featurecounts_cmd} '
        ' -F GTF -B -C -p --primary -O '
        ' -a {input.annotation} -g exon_id '
        ' -o  {output} {input.bam}'


rule split_n_trim_gatk:
    input:
        fasta=fasta_unzipped,
        fai=fasta_idx,
        fa_dict=fasta_dict,
        bam='star/{sample}/{sample}.2pass.Aligned.sortedByCoord.dedup.bam'
    output:
        temp('star/{sample}/{sample}.2pass.Aligned.sortedByCoord.split.bam')
    shell:
        '{gatk_cmd} SplitNCigarReads -R {input.fasta} -I {input.bam} -O {output}'


## IndelRealigner is no longer included in GATK as of version 4.0.0.0
# rule indel_realignment_gatk:
#     input:
#         fasta=fasta_unzipped,
#         bam='star/{sample}/{sample}.2pass.Aligned.sortedByCoord.split.bam',
#         targetIntervals=reAlignmentIntervals,
#         known1=knownIndelsMills,
#         known2=knownIndels100G
#     output:
#         'star/{sample}/{sample}.2pass.Aligned.sortedByCoord.split.realigned.bam'
#     shell:
#         '{gatk_cmd} -T IndelRealigner -R {input.fasta} -I {input.bam} '
#         '-targetIntervals {input.targetIntervals} -known {input.known1} -known {input.known2} '
#         '-U ALLOW_N_CIGAR_READS --consensusDeterminationModel KNOWNS_ONLY --LODThresholdForCleaning 0.4  '
#         '-o {output}'


rule call_variants_mpileup_fthicov:
    input:
        fasta=fasta_unzipped,
        fai=fasta_idx,
        fa_dict=fasta_dict,
        dbSnp=dbSnpVcf,
        dbSnpSmall= dbSnpVcfSmall,
        targets=targets_callvars,
        bam='star/{sample}/{sample}.2pass.Aligned.sortedByCoord.split.bam'
        # bam='star/{sample}/{sample}.2pass.Aligned.sortedByCoord.split.realigned.bqsr.bam'
    output:
        mpi=temp('vcf/{sample}/{sample}.filtered.bcf.gz'),
        midx=temp('vcf/{sample}/{sample}.filtered.bcf.gz.csi'),
        vcf='vcf/{sample}/{sample}.filtered.vcf.gz',
        idx='vcf/{sample}/{sample}.filtered.vcf.gz.csi'
    shell:
        """
        mkdir tmp/tmp-{wildcards.sample}
        {bcftools_cmd} mpileup -E -Ob --skip-indels -R {input.dbSnpSmall} \
        -f {input.fasta} --annotate AD,DP,SP,INFO/AD {input.bam} -o {output.mpi}
        {bcftools_cmd} index {output.mpi}
        {bcftools_cmd} call -R {input.dbSnpSmall} -T {input.targets} -m -Ou {output.mpi} | \
        {bcftools_cmd} filter -Ou -i'DP>3 && QUAL>20' | \
        {bcftools_cmd} sort -T tmp/tmp-{wildcards.sample} --max-mem 2G -Oz -o {output.vcf}
        {bcftools_cmd} index {output.vcf}
        """


rule filter_hipsci_overlap_variants:
    input:
        sc_vcf='vcf/{sample}/{sample}.filtered.vcf.gz',
        sc_vcf_idx='vcf/{sample}/{sample}.filtered.vcf.gz.csi',
        hipsci_vcf=HIPSCI_VCF
    output:
        lst=temp('vcf/{sample}/{sample}.variant_list.txt'),
        tmp=temp('vcf/{sample}/{sample}.tmp.vcf.gz'),
        vcf=temp('vcf/{sample}/{sample}.filtered.hipsci.overlap.vcf.gz'),
        csi=temp('vcf/{sample}/{sample}.filtered.hipsci.overlap.vcf.gz.csi')
    shell:
        """
        set +euo pipefail
        echo -e "1\\t1\\tA\\tC" > {output}
        {bcftools_cmd} view -O v {input.sc_vcf} | grep -v ^# | awk \'{{sub(/chr/,""); print $1"\\t"$2"\\t"$4"\\t"$5}}\' >> {output.lst}
        {bcftools_cmd} view -o {output.tmp} -O z -l 9 -R {output.lst} {input.hipsci_vcf}
        vcf-sort {output.tmp} | bgzip -c > {output.vcf}
        {bcftools_cmd} index {output.vcf}
        set -euo pipefail
        """


rule identify_donor_runs:
    input:
        sc_vcf='vcf/{sample}/{sample}.filtered.vcf.gz',
        hipsci_vcf='vcf/{sample}/{sample}.filtered.hipsci.overlap.vcf.gz',
        hipsci_vcf_idx='vcf/{sample}/{sample}.filtered.hipsci.overlap.vcf.gz.csi'
    output:
        'donor_id/{sample}.donor_id.csv'
    params:
        prefix='donor_id/{sample}.donor_id'
    shell:
        '{rscript_cmd} ../../../src/R/identify_donor_small_vcf_cardelino.R '
        '--input_file "{input.sc_vcf}" '
        '--donor_lines "vass;wuye;wetu" '
        '--donor_vcf {input.hipsci_vcf} '
        '--output_prefix "{params.prefix}" '


rule identify_donor_collect:
    input:
        files=variant_donor_id_files
    output:
        'donor_id_all.csv'
    params:
        dir='donor_id/'
    run:
        import pandas as pd
        import glob
        import os
        input_files = glob.glob(os.path.join(params.dir, '*.donor_id.csv'))
        df_list = {}
        for infile in input_files:
            df_tmp = pd.read_csv(infile)
            df_list[infile] = df_tmp
        df_out = pd.concat(df_list)
        df_out.to_csv(output[0], index=False)


rule merge_cell_vcfs:
    input:
        files=cell_vcf_files
    output:
        'cells_merged.vcf.gz'
    params:
        dir='vcf'
    shell:
        """
        set +euo pipefail
        {bcftools_cmd} merge -O z -o {output} $(ls -1 {params.dir}/*/*.filtered.vcf.gz | perl -pe 's/\n/ /g')
        set -euo pipefail
        """


rule salmon_to_sceset_GRCh37:
    input:
        files=salmon_results_GRCh37
    output:
        'sces/sce.salmon.GRCh37.preqc_tx.rds',
        'sces/sce.salmon.GRCh37.preqc_gene.rds'
    params:
        input_dir='quant_salmon',
        output_prefix='sces/sce.salmon.GRCh37.preqc'
    shell:
        '{rscript_cmd} {read_salmon_to_scesets_cmd} '
        '--input_dir {params.input_dir} '
        '--output_prefix {params.output_prefix} '
        '--biomart feb2014.archive.ensembl.org'


rule salmon_to_sceset_GRCh38:
    input:
        files=salmon_results_GRCh38
    output:
        'sces/sce.salmon.GRCh38.preqc_tx.rds',
        'sces/sce.salmon.GRCh38.preqc_gene.rds',
        'sces/sce.salmon.GRCh38.preqc.feather'
    params:
        input_dir='quant_salmon_GRCh38',
        output_prefix='sces/sce.salmon.GRCh38.preqc'
    shell:
        '{rscript_cmd} {read_salmon_to_scesets_cmd} '
        '--input_dir {params.input_dir} '
        '--output_prefix {params.output_prefix} '
        '--biomart jul2016.archive.ensembl.org'


rule rough_qc:
    input:
        'sces/sce.salmon.{genome}.preqc_gene.rds'
    output:
        'first_qc/salmon.{genome}.first_qc.html'
    shell:
        '{rscript_cmd} ../../../src/R/compile_report.R '
        '-i {input} -o {output} '
        '--template ../../../src/Rmd/rough_qc_template.Rmd '


# rule fastqc_reports:
#     input:
#         'Data/{run}/star/{sample}/{sample}.2pass.Aligned.sortedByCoord.split.realigned.bqsr.bam'
#     output:
#         'Data/{run}/star/{sample}/{sample}.2pass.Aligned.sortedByCoord.out.bam_fastqc.html'
#     params:
#         output_dir="Data/{run}/fastqc/"
#     shell:
#         '/nfs/software/stegle/FastQC/fastqc -o {params.output_dir} {input}'

## Merging bams to create super-bam for variant calling
#  ls 21*/star/*/*sortedByCoord.split.realigned.bqsr.bam > bam_list.txt
# bamtools merge -list bam_list.txt -out ss2_merged.bam

## Call variants for all cells individually, but in one file
# /nfs/software/stegle/users/dseaton/java/jdk1.8.0_112/bin/java -Xmx50g -Xms8g -Djava.io.tmpdir=/hps/nobackup/hipsci/scratch/singlecell_endodiff/tmp -jar /hps/nobackup/stegle/users/mjbonder/tools/GATK/GenomeAnalysisTK.jar -T HaplotypeCaller -R /hps/nobackup/stegle/datasets/references/human/STAR_GRCh37.75_ERCC/GRCh37.p13.genome.ERCC92.fa -I ss2_new_merged.bam -dontUseSoftClippedBases -stand_call_conf 20.0 -o ss2_new_merged_bam_output.vcf
## change readgroups to pool reads across cells
# /nfs/software/stegle/users/dseaton/java/jdk1.8.0_112/bin/java -jar /nfs/software/stegle/users/dseaton/picard/picard.jar AddOrReplaceReadGroups I=ss2_merged.bam O=ss2_merged_repRG.bam SO=coordinate RGID=22379 RGLB=22379 RGSM=22379 RGPL=illumina RGPU=sanger
## call variants for pooled reads
# /nfs/software/stegle/users/dseaton/java/jdk1.8.0_112/bin/java -Xmx50g -Xms8g -Djava.io.tmpdir=/hps/nobackup/hipsci/scratch/singlecell_endodiff/tmp -jar /hps/nobackup/stegle/users/mjbonder/tools/GATK/GenomeAnalysisTK.jar -T HaplotypeCaller -R /hps/nobackup/stegle/datasets/references/human/STAR_GRCh37.75_ERCC/GRCh37.p13.genome.ERCC92.fa -I ss2_new_merged_repRG.bam -dontUseSoftClippedBases -stand_call_conf 20.0 -o ss2_new_merged_bam_pooled.vcf

